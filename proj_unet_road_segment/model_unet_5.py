# -*- coding: utf-8 -*-
"""model_unet_5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17G9ikRGNCdqGUkYVw3ztyzoOnUsr7rnA
"""

# -*- coding: utf-8 -*-
"""
(Modified for a 5-block Vanilla U-Net)

"""

################################## LIBRARIES ##################################

from tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation,
                                     UpSampling2D, concatenate, Lambda,
                                     MaxPooling2D)
from tensorflow.keras import Model, Input
import tensorflow as tf

################################ DOUBLE CONVOLUTIONAL BLOCK ##################################
# Each block applies two sequential convolution layers with batch normalization
# and ReLU activation (no residual addition).

def conv_block(feature_map, filters):
    conv_1 = Conv2D(filters, kernel_size=(3, 3), padding='same')(feature_map)
    bn1 = BatchNormalization()(conv_1)
    act1 = Activation('relu')(bn1)

    conv_2 = Conv2D(filters, kernel_size=(3, 3), padding='same')(act1)
    bn2 = BatchNormalization()(conv_2)
    act2 = Activation('relu')(bn2)

    return act2

###################################### ENCODER ######################################
# This encoder now uses 5 blocks. It downsamples the input by applying
# conv_block and MaxPooling2D sequentially and stores the intermediate feature maps.

def encoder(feature_map):
    skips = []  # To store feature maps for skip connections

    # Block 1: 64 filters
    conv1 = conv_block(feature_map, filters=64)
    skips.append(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    # Block 2: 128 filters
    conv2 = conv_block(pool1, filters=128)
    skips.append(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    # Block 3: 256 filters
    conv3 = conv_block(pool2, filters=256)
    skips.append(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    # Block 4: 512 filters
    conv4 = conv_block(pool3, filters=512)
    skips.append(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    # Block 5: 1024 filters
    conv5 = conv_block(pool4, filters=1024)
    skips.append(conv5)
    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)

    return skips, pool5

###################################### BOTTLENECK ######################################
# The bottleneck is a conv_block with 1024 filters.

def bottleneck(pool):
    return conv_block(pool, filters=1024)

###################################### DECODER ######################################
# The decoder upsamples and concatenates with the corresponding skip connections.
# Now it has five decoding blocks that mirror the encoder's depth.

def decoder(feature_map, skips):
    # Block 1: Up-sample and concatenate with block 5's features (1024 filters)
    up1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(feature_map)
    concat1 = concatenate([up1, skips[4]], axis=3)
    dec1 = conv_block(concat1, filters=1024)

    # Block 2: Up-sample and concatenate with block 4's features (512 filters)
    up2 = UpSampling2D(size=(2, 2), interpolation='bilinear')(dec1)
    concat2 = concatenate([up2, skips[3]], axis=3)
    dec2 = conv_block(concat2, filters=512)

    # Block 3: Up-sample and concatenate with block 3's features (256 filters)
    up3 = UpSampling2D(size=(2, 2), interpolation='bilinear')(dec2)
    concat3 = concatenate([up3, skips[2]], axis=3)
    dec3 = conv_block(concat3, filters=256)

    # Block 4: Up-sample and concatenate with block 2's features (128 filters)
    up4 = UpSampling2D(size=(2, 2), interpolation='bilinear')(dec3)
    concat4 = concatenate([up4, skips[1]], axis=3)
    dec4 = conv_block(concat4, filters=128)

    # Block 5: Up-sample and concatenate with block 1's features (64 filters)
    up5 = UpSampling2D(size=(2, 2), interpolation='bilinear')(dec4)
    concat5 = concatenate([up5, skips[0]], axis=3)
    dec5 = conv_block(concat5, filters=64)

    return dec5

###################################### VANILLA U-NET ######################################
# The complete U-Net model includes an input normalization step, followed
# by the encoder, bottleneck, decoder, and a final 1x1 convolution with sigmoid activation.

def UNet(inputshape):
    # Input layer and normalization (scale input to 0-1)
    model_input = Input(shape=inputshape)
    model_input_norm = Lambda(lambda x: x / 255)(model_input)

    # Encoder: get skip connections and the final pooled output.
    skips, pool = encoder(model_input_norm)

    # Bottleneck layer.
    bn = bottleneck(pool)

    # Decoder: upsample using stored skip connections.
    dec = decoder(bn, skips)

    # Output layer: 1 filter with sigmoid activation for binary segmentation.
    model_output = Conv2D(filters=1, kernel_size=(1, 1), padding='same',
                          activation='sigmoid')(dec)

    return Model(model_input, model_output)

################################ SANITY CHECK #################################
if __name__ == "__main__":
    model = UNet((224, 224, 3))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.summary()
    # Optionally, you could plot the model architecture:
    # tf.keras.utils.plot_model(model, to_file='unet_model.png', show_layer_names=True,
    #                           show_shapes=True, rankdir='TB')