# -*- coding: utf-8 -*-
"""train_fix_JN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhZKlO4rTjjwpb_x1P2xH-qavNxVV9Ux
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Jul  8 15:42:46 2020

@author: edwin.p.alegre
"""

import os

abspath = os.path.abspath(__file__)
dname = os.path.dirname(abspath)
os.chdir(dname)

import model_unet_5
import numpy as np
import tensorflow as tf
from math import floor
from tqdm import tqdm
from skimage.io import imread
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow_addons.metrics import F1Score
import random
import scipy.misc
from PIL import Image
import shutil
# from utils import imgstitch, DatasetLoad
from utils_fix_jn import imgstitch, DatasetLoad


########################### LEARNING RATE SCHEDULER ###########################

# Function for learning rate decay. The learning rate will reduce by a factor of 0.1 every 10 epochs.
def schedlr(epoch, lr):
    new_lr = 0.001 * (0.1)**(floor(epoch/10))
    return new_lr

############################### HYPERPARAMETERS ###############################

IMG_SIZE = 224
BATCH = 8
EPOCHS = 30

################################### DATASET ###################################

# Paths for relevant datasets to load in
train_dataset = r'/projectnb/ec523/projects/Proj_road_segment_fix/EE8204-ResUNet/dataset/samples_train'
test_dataset = r'/projectnb/ec523/projects/Proj_road_segment_fix/EE8204-ResUNet/dataset/test_patch'
#val_dataset = r'dataset/samples_val'

# Make a list of the test folders to be used when predicting the model. This will be fed into the prediction
# flow to generate the stitched image based off the predictions of the patches fed into the network
_, test_fol, _ = next(os.walk(test_dataset))

# Load in the relevant datasets
if os.path.exists("preprocessed_data.npz"):
    data = np.load("preprocessed_data.npz", allow_pickle = True)
    X_train = data["X_train"]
    Y_train = data["Y_train"]
    X_test  = data["X_test"]
    Y_test  = data["Y_test"]
    X_val   = data["X_val"]
    Y_val   = data["Y_val"]
else:
    X_train, Y_train, X_test, Y_test, X_val, Y_val = DatasetLoad(train_dataset, test_dataset)
    np.savez_compressed("preprocessed_data.npz",
                        X_train=X_train, Y_train=Y_train,
                        X_test=X_test, Y_test=Y_test,
                        X_val=X_val, Y_val=Y_val)
################################ RESIDUAL UNET ################################

sgd_optimizer = Adam()

# Metrics to be used when evaluating the network
precision = tf.keras.metrics.Precision()
recall = tf.keras.metrics.Recall()
f1 = F1Score(num_classes=2, name='f1', average='micro', threshold=0.4)

# Instantiate the network
'''
model = model_unet.UNet((IMG_SIZE, IMG_SIZE, 3))
model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy', precision, recall, f1])
model.summary()

# Callacks to be used in the network. Checkpoint can be adjusted to save the best (lowest loss) if desired.
checkpoint_path = os.path.join(dname, 'models', 'unet.{epoch:02d}-{f1:.2f}.hdf5')
checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=False)

callbacks =[
    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),
    tf.keras.callbacks.TensorBoard(log_dir='logs'),
    LearningRateScheduler(schedlr, verbose=1),
    checkpoint]
print("Y_train shape:", Y_train.shape, "dtype:", Y_train.dtype, "min:", np.min(Y_train), "max:", np.max(Y_train))
# Fit the network to the training dataset. The validation dataset can be used instead of a validataion split
model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    batch_size=BATCH,
    epochs=EPOCHS,
    callbacks=callbacks
)
'''
# Uncomment lines 84-85 and comment line 78 to run a previous model for prediction. Uncommenting lines 84-86 will
# allow for training continuation in the event that the training was interuppted for whatever reason. If this is the
# case, please comment out line 78 as well


latest_checkpoint = r'models/unet.16-0.91.hdf5'
model = tf.keras.models.load_model(latest_checkpoint)
'''
callbacks =[
    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),
    tf.keras.callbacks.TensorBoard(log_dir='logs'),
    LearningRateScheduler(schedlr, verbose=1),
    latest_checkpoint]
'''
#model.fit(X_train, Y_train, validation_split=0.1, batch_size=BATCH, epochs=EPOCHS, callbacks=callbacks, initial_epoch=8)

########################### PREDICTION AND RESULTS ############################

# If previous results exist, delete them so the results won't be mixed up
if os.path.isdir(r'results_5') == True:
    shutil.rmtree('results_5')

# Make new results directory along with sub directories for each of the test images
if os.path.isdir(r'results_5') == False:
    os.mkdir('results_5')
# Flattening the dictionary to a single array
# X_test is an object array, convert back to dict if needed
   ### TESTING DATASET ###
    # Get list of test folders
    _, test_fol, _ = next(os.walk(test_dataset))
    # Assume all test folders have the same number of images, using the first folder to determine that number.
    _, _, test_files = next(os.walk(os.path.join(test_dataset, test_fol[0], 'image')))
    test_imgs = len(test_files)
    test_ids = list(range(1, test_imgs + 1))

    X_test = {}
    Y_test = {}

    # Create arrays for each test folder
    for folder in test_fol:
        X_test[folder] = np.zeros((len(test_ids), IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)
        Y_test[folder] = np.zeros((len(test_ids), IMG_SIZE, IMG_SIZE, 1), dtype=np.bool)

    # Load testing images and masks for each
    for folder in test_fol:
        test_path = os.path.join(test_dataset, folder)
        for n, id_ in tqdm(enumerate(test_files), total=len(test_files)):
            X_test[folder][n] = imread(os.path.join(test_path, 'image', f"{id_}"))
            mask = np.zeros((IMG_SIZE, IMG_SIZE, 1), dtype=np.bool)
            for _ in next(os.walk(os.path.join(test_path, 'mask'))):
                mask_ = imread(os.path.join(test_path, 'mask', f"{id_}"))
                mask_ = np.expand_dims(mask_, axis=-1)
                mask = np.maximum(mask, mask_)
            Y_test[folder][n] = mask

for i in test_fol:
    if os.path.isdir('results_5/%s' % i) == False:
        os.mkdir('results_5/%s' % i)

    save_dir = os.path.join('results_5', str(i))
    print(type(i), i)

    pred_test = model.predict(X_test[i], verbose=1)
    pred_test_mask = (pred_test > 0.4).astype(np.uint8)

    # for n in range(len(pred_test_mask)):
    #     outputmask = np.squeeze(pred_test_mask[n]*255)
    #     saveimg = Image.fromarray(outputmask, 'L')
    #     saveimg.save(os.path.join(save_dir, str(n)).replace('\\','/') + '.png', 'PNG')

    for n, fname in enumerate(test_files):
      outputmask = np.squeeze(pred_test_mask[n] * 255)
      saveimg = Image.fromarray(outputmask.astype(np.uint8), 'L')
      saveimg.save(os.path.join(save_dir, fname), 'PNG')

# Loop through the entire test prediction dataset and feed the images as an input to the stitching function
for i in test_fol:
    results_dir = os.path.join('results_5', str(i))
    imgstitch(results_dir)